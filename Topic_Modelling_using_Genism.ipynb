{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject  ...  zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0  ...    0    0         0     0        0   \n",
       "anthony     0        0       0  ...    0    0         0     0        0   \n",
       "bill        1        0       0  ...    0    0         0     1        1   \n",
       "bo          0        1       0  ...    0    0         0     1        0   \n",
       "dave        0        0       0  ...    0    0         0     0        0   \n",
       "hasan       0        0       0  ...    2    1         0     1        0   \n",
       "jim         0        0       0  ...    0    0         0     0        0   \n",
       "joe         0        0       0  ...    0    0         0     0        0   \n",
       "john        0        0       0  ...    0    0         0     0        0   \n",
       "louis       0        0       0  ...    0    0         0     2        0   \n",
       "mike        0        0       0  ...    0    0         2     1        0   \n",
       "ricky       0        1       1  ...    0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  éclair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7468 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from gensim) (1.16.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\mitta\\appdata\\roaming\\python\\python37\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.48.tar.gz (97 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Collecting botocore<1.18.0,>=1.17.48\n",
      "  Downloading botocore-1.17.48-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mitta\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110323 sha256=17cd23d2b0f0243ae45ff98403784cfc506376a91d833d036df3fa9f4971e0cd\n",
      "  Stored in directory: c:\\users\\mitta\\appdata\\local\\pip\\cache\\wheels\\56\\b5\\6d\\86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\n",
      "  Building wheel for boto3 (setup.py): started\n",
      "  Building wheel for boto3 (setup.py): finished with status 'done'\n",
      "  Created wheel for boto3: filename=boto3-1.14.48-py2.py3-none-any.whl size=127855 sha256=b934a4ef2ef13eafc12dfeb43b991d20e11c6588f750788d8f693fe1d51fef62\n",
      "  Stored in directory: c:\\users\\mitta\\appdata\\local\\pip\\cache\\wheels\\b0\\f8\\d9\\d2a816eb3b5fed353cf3e7746d2c3bd0cac694045d063a2b53\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.13\n",
      "    Uninstalling Cython-0.29.13:\n",
      "      Successfully uninstalled Cython-0.29.13\n",
      "Successfully installed Cython-0.29.14 boto3-1.14.48 botocore-1.17.48 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>aaaaah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaaahhhhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaaauuugghhhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaahhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) # corpus is a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitta\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3681: 'ladies',\n",
       " 2751: 'gentlemen',\n",
       " 7249: 'welcome',\n",
       " 6272: 'stage',\n",
       " 157: 'ali',\n",
       " 7356: 'wong',\n",
       " 3096: 'hi',\n",
       " 3075: 'hello',\n",
       " 6661: 'thank',\n",
       " 1355: 'coming',\n",
       " 5910: 'shit',\n",
       " 1042: 'cause',\n",
       " 4796: 'pee',\n",
       " 4202: 'minutes',\n",
       " 2271: 'everybody',\n",
       " 6974: 'um',\n",
       " 2295: 'exciting',\n",
       " 1707: 'day',\n",
       " 7422: 'year',\n",
       " 6924: 'turned',\n",
       " 7431: 'yes',\n",
       " 283: 'appreciate',\n",
       " 6970: 'uh',\n",
       " 6617: 'tell',\n",
       " 2765: 'getting',\n",
       " 4565: 'older',\n",
       " 2781: 'girl',\n",
       " 412: 'automatic',\n",
       " 6695: 'thought',\n",
       " 2665: 'fuck',\n",
       " 6375: 'straight',\n",
       " 3492: 'jealous',\n",
       " 2597: 'foremost',\n",
       " 4147: 'metabolism',\n",
       " 2784: 'girls',\n",
       " 2102: 'eat',\n",
       " 6021: 'sixpack',\n",
       " 6665: 'thatthat',\n",
       " 551: 'beautiful',\n",
       " 3365: 'inner',\n",
       " 6683: 'thigh',\n",
       " 1245: 'clearance',\n",
       " 2429: 'feet',\n",
       " 6677: 'theres',\n",
       " 3228: 'huge',\n",
       " 2708: 'gap',\n",
       " 3816: 'light',\n",
       " 5032: 'potential',\n",
       " 5261: 'radiating',\n",
       " 6712: 'throughand',\n",
       " 6055: 'sleep',\n",
       " 3378: 'insomnia',\n",
       " 195: 'ambien',\n",
       " 1992: 'download',\n",
       " 4103: 'meditation',\n",
       " 4520: 'oasis',\n",
       " 4959: 'podcast',\n",
       " 949: 'calm',\n",
       " 1120: 'chatter',\n",
       " 5394: 'regret',\n",
       " 5461: 'resentment',\n",
       " 2380: 'family',\n",
       " 1295: 'cluttering',\n",
       " 4184: 'mind',\n",
       " 3849: 'lives',\n",
       " 122: 'ahead',\n",
       " 3223: 'hpv',\n",
       " 4784: 'peace',\n",
       " 4454: 'night',\n",
       " 4562: 'ok',\n",
       " 1339: 'come',\n",
       " 2671: 'fucking',\n",
       " 3895: 'loser',\n",
       " 5706: 'says',\n",
       " 3900: 'lot',\n",
       " 4127: 'men',\n",
       " 6998: 'undetectable',\n",
       " 5329: 'really',\n",
       " 2666: 'fucked',\n",
       " 2768: 'ghost',\n",
       " 3373: 'inside',\n",
       " 4129: 'mens',\n",
       " 722: 'bodies',\n",
       " 735: 'boo',\n",
       " 7350: 'womens',\n",
       " 1945: 'doctor',\n",
       " 6768: 'told',\n",
       " 6377: 'strains',\n",
       " 3632: 'kind',\n",
       " 6923: 'turn',\n",
       " 1071: 'cervical',\n",
       " 966: 'cancer',\n",
       " 723: 'body',\n",
       " 3044: 'heal',\n",
       " 3081: 'helpful',\n",
       " 518: 'basically',\n",
       " 1852: 'die',\n",
       " 5063: 'presence',\n",
       " 7345: 'wolverine',\n",
       " 655: 'bitches',\n",
       " 3635: 'kindle',\n",
       " 6925: 'turning',\n",
       " 5811: 'selfhelp',\n",
       " 3802: 'library',\n",
       " 3403: 'interested',\n",
       " 740: 'books',\n",
       " 5867: 'shades',\n",
       " 2887: 'grey',\n",
       " 3810: 'lifechanging',\n",
       " 3960: 'magic',\n",
       " 6727: 'tidying',\n",
       " 1742: 'declutter',\n",
       " 3153: 'home',\n",
       " 42: 'achieve',\n",
       " 4614: 'optimum',\n",
       " 3786: 'level',\n",
       " 6449: 'success',\n",
       " 3193: 'horrible',\n",
       " 4834: 'person',\n",
       " 2998: 'happy',\n",
       " 3079: 'help',\n",
       " 6782: 'tony',\n",
       " 5553: 'robbins',\n",
       " 4110: 'mei',\n",
       " 3137: 'hoarding',\n",
       " 5105: 'problem',\n",
       " 3184: 'hoping',\n",
       " 1059: 'center',\n",
       " 5106: 'problems',\n",
       " 2814: 'goes',\n",
       " 429: 'away',\n",
       " 1888: 'disappear',\n",
       " 4252: 'mom',\n",
       " 7378: 'world',\n",
       " 1524: 'country',\n",
       " 6589: 'taught',\n",
       " 6713: 'throw',\n",
       " 1847: 'dictators',\n",
       " 4664: 'overtake',\n",
       " 6115: 'snatch',\n",
       " 7220: 'wealth',\n",
       " 602: 'better',\n",
       " 3140: 'hold',\n",
       " 5484: 'retainer',\n",
       " 2846: 'grade',\n",
       " 2980: 'handy',\n",
       " 5946: 'shovel',\n",
       " 913: 'busy',\n",
       " 6432: 'stuffing',\n",
       " 2816: 'gold',\n",
       " 915: 'butt',\n",
       " 5626: 'running',\n",
       " 1370: 'communiststhe',\n",
       " 5663: 'san',\n",
       " 2626: 'francisco',\n",
       " 6906: 'trying',\n",
       " 5519: 'rid',\n",
       " 7386: 'worst',\n",
       " 2324: 'experience',\n",
       " 3809: 'life',\n",
       " 2183: 'emotional',\n",
       " 5749: 'screaming',\n",
       " 2465: 'fighting',\n",
       " 7427: 'yelling',\n",
       " 954: 'came',\n",
       " 1256: 'climax',\n",
       " 5385: 'refused',\n",
       " 3780: 'let',\n",
       " 6656: 'texas',\n",
       " 3392: 'instruments',\n",
       " 4007: 'manual',\n",
       " 940: 'calculator',\n",
       " 5104: 'probably',\n",
       " 486: 'bamboozled',\n",
       " 2738: 'generation',\n",
       " 5455: 'required',\n",
       " 922: 'buy',\n",
       " 1510: 'cost',\n",
       " 3560: 'judy',\n",
       " 3516: 'jetsons',\n",
       " 3702: 'laptop',\n",
       " 2694: 'future',\n",
       " 2870: 'graph',\n",
       " 6642: 'tesla',\n",
       " 4406: 'need',\n",
       " 1240: 'clean',\n",
       " 5111: 'procrastinator',\n",
       " 258: 'anymore',\n",
       " 36: 'according',\n",
       " 1747: 'deepakoprah',\n",
       " 7216: 'way',\n",
       " 2886: 'grew',\n",
       " 4758: 'past',\n",
       " 4028: 'married',\n",
       " 3987: 'man',\n",
       " 3924: 'lucky',\n",
       " 2927: 'guy',\n",
       " 2725: 'gave',\n",
       " 2600: 'forever',\n",
       " 1694: 'dated',\n",
       " 3896: 'losers',\n",
       " 3901: 'lots',\n",
       " 6027: 'skaters',\n",
       " 7183: 'wanna',\n",
       " 2902: 'grownass',\n",
       " 7346: 'woman',\n",
       " 6364: 'stop',\n",
       " 1697: 'dating',\n",
       " 7013: 'unless',\n",
       " 7168: 'wake',\n",
       " 4068: 'mattress',\n",
       " 3645: 'kitchen',\n",
       " 6681: 'theyre',\n",
       " 5863: 'sexy',\n",
       " 4648: 'outside',\n",
       " 3984: 'malt',\n",
       " 3835: 'liquor',\n",
       " 3251: 'husband',\n",
       " 4146: 'met',\n",
       " 7232: 'wedding',\n",
       " 3092: 'hes',\n",
       " 3884: 'looking',\n",
       " 3744: 'league',\n",
       " 5699: 'saw',\n",
       " 2809: 'god',\n",
       " 6685: 'thing',\n",
       " 3749: 'learned',\n",
       " 388: 'attending',\n",
       " 3014: 'harvard',\n",
       " 910: 'business',\n",
       " 5724: 'school',\n",
       " 6846: 'trap',\n",
       " 347: 'ass',\n",
       " 2815: 'going',\n",
       " 6847: 'trapped',\n",
       " 3361: 'initially',\n",
       " 3644: 'kissing',\n",
       " 2459: 'fifth',\n",
       " 1693: 'date',\n",
       " 7028: 'unusual',\n",
       " 1850: 'did',\n",
       " 5195: 'purpose',\n",
       " 3654: 'knew',\n",
       " 1029: 'catch',\n",
       " 2835: 'gotta',\n",
       " 3976: 'make',\n",
       " 2058: 'dude',\n",
       " 576: 'believe',\n",
       " 5780: 'secret',\n",
       " 2711: 'garden',\n",
       " 5170: 'public',\n",
       " 4727: 'park',\n",
       " 3204: 'hosted',\n",
       " 5390: 'reggae',\n",
       " 2446: 'fests',\n",
       " 33: 'accidentally',\n",
       " 3155: 'homeless',\n",
       " 3123: 'hipsters',\n",
       " 6369: 'store',\n",
       " 7044: 'urban',\n",
       " 4643: 'outfitters',\n",
       " 6686: 'things',\n",
       " 1426: 'confusing',\n",
       " 3122: 'hipster',\n",
       " 542: 'beard',\n",
       " 2399: 'fashion',\n",
       " 7192: 'warmth',\n",
       " 2991: 'happened',\n",
       " 3852: 'living',\n",
       " 832: 'broad',\n",
       " 1709: 'daylight',\n",
       " 1144: 'chemistry',\n",
       " 3094: 'hey',\n",
       " 7204: 'wassup',\n",
       " 7148: 'volvo',\n",
       " 2036: 'drop',\n",
       " 2037: 'dropped',\n",
       " 2817: 'golden',\n",
       " 2721: 'gate',\n",
       " 7208: 'watched',\n",
       " 5623: 'run',\n",
       " 4167: 'middle',\n",
       " 2652: 'friends',\n",
       " 336: 'asian',\n",
       " 5920: 'shocked',\n",
       " 7059: 'usually',\n",
       " 337: 'asianamerican',\n",
       " 7349: 'women',\n",
       " 7223: 'wear',\n",
       " 3633: 'kinda',\n",
       " 2795: 'glasses',\n",
       " 4609: 'opinions',\n",
       " 7285: 'white',\n",
       " 2059: 'dudes',\n",
       " 4415: 'neighborhood',\n",
       " 3974: 'major',\n",
       " 1217: 'city',\n",
       " 197: 'america',\n",
       " 7438: 'yoko',\n",
       " 4593: 'ono',\n",
       " 2358: 'factory',\n",
       " 7266: 'whats',\n",
       " 7407: 'wrong',\n",
       " 2425: 'feel',\n",
       " 4887: 'picturesque',\n",
       " 7254: 'wes',\n",
       " 215: 'anderson',\n",
       " 4311: 'movie',\n",
       " 6596: 'teach',\n",
       " 1483: 'cool',\n",
       " 6430: 'stuff',\n",
       " 7151: 'voting',\n",
       " 5362: 'recycling',\n",
       " 1934: 'disturbing',\n",
       " 1948: 'documentaries',\n",
       " 3415: 'introduce',\n",
       " 3205: 'hot',\n",
       " 3176: 'hookin',\n",
       " 4087: 'mean',\n",
       " 3978: 'makes',\n",
       " 5042: 'powerful',\n",
       " 2105: 'eats',\n",
       " 5208: 'pussy',\n",
       " 19: 'absorbing',\n",
       " 5101: 'privilege',\n",
       " 2226: 'entitlement',\n",
       " 4259: 'money',\n",
       " 3143: 'hole',\n",
       " 7154: 'vulnerable',\n",
       " 1610: 'crush',\n",
       " 3033: 'head',\n",
       " 4253: 'moment',\n",
       " 3624: 'kill',\n",
       " 789: 'brains',\n",
       " 1331: 'colonize',\n",
       " 1332: 'colonizer',\n",
       " 3661: 'knowbut',\n",
       " 4026: 'marriage',\n",
       " 4446: 'nice',\n",
       " 6151: 'somebody',\n",
       " 5254: 'race',\n",
       " 85: 'advantage',\n",
       " 5260: 'racist',\n",
       " 5702: 'say',\n",
       " 2332: 'explain',\n",
       " 2949: 'halffilipino',\n",
       " 2951: 'halfjapanese',\n",
       " 2947: 'halfchinese',\n",
       " 2954: 'halfvietnamese',\n",
       " 6210: 'spend',\n",
       " 4823: 'percent',\n",
       " 5915: 'shitting',\n",
       " 3668: 'korean',\n",
       " 193: 'amazing',\n",
       " 3910: 'love',\n",
       " 870: 'built',\n",
       " 3664: 'knowmy',\n",
       " 778: 'boyfriend',\n",
       " 1616: 'cuban',\n",
       " 4157: 'mexican',\n",
       " 2928: 'guys',\n",
       " 300: 'arent',\n",
       " 6992: 'underrated',\n",
       " 5857: 'sexiest',\n",
       " 2939: 'hair',\n",
       " 4404: 'neck',\n",
       " 3980: 'making',\n",
       " 1962: 'dolphin',\n",
       " 6107: 'smooth',\n",
       " 6071: 'slip',\n",
       " 6062: 'slide',\n",
       " 665: 'black',\n",
       " 2504: 'fish',\n",
       " 6734: 'tilikum',\n",
       " 554: 'bed',\n",
       " 4598: 'oohwee',\n",
       " 4140: 'mess',\n",
       " 3518: 'jewish',\n",
       " 5363: 'red',\n",
       " 3350: 'inflamed',\n",
       " 340: 'ask',\n",
       " 2304: 'exfoliated',\n",
       " 6761: 'today',\n",
       " 3491: 'jdate',\n",
       " 3881: 'loofah',\n",
       " 6662: 'thanks',\n",
       " 5613: 'rug',\n",
       " 899: 'burn',\n",
       " 421: 'avi',\n",
       " 4544: 'odor',\n",
       " 6093: 'smell',\n",
       " 5477: 'responsibility',\n",
       " 6975: 'umami',\n",
       " 2533: 'flavor',\n",
       " 1348: 'comes',\n",
       " 2661: 'fromi',\n",
       " 7026: 'unspoken',\n",
       " 6994: 'understanding',\n",
       " 2948: 'halffancy',\n",
       " 2952: 'halfjungle',\n",
       " 1857: 'difference',\n",
       " 2386: 'fancy',\n",
       " 338: 'asians',\n",
       " 1173: 'chinese',\n",
       " 3484: 'japanese',\n",
       " 3203: 'host',\n",
       " 4574: 'olympics',\n",
       " 3572: 'jungle',\n",
       " 1907: 'diseases',\n",
       " 1859: 'different',\n",
       " 2098: 'east',\n",
       " 1299: 'coast',\n",
       " 5100: 'private',\n",
       " 4943: 'playing',\n",
       " 3679: 'lacrosse',\n",
       " 3750: 'learning',\n",
       " 3715: 'latin',\n",
       " 1149: 'chess',\n",
       " 5614: 'rugby',\n",
       " 2470: 'filipino',\n",
       " 1002: 'carlton',\n",
       " 1851: 'didnt',\n",
       " 7120: 'vietnamese',\n",
       " 1696: 'dates',\n",
       " 6783: 'took',\n",
       " 5480: 'restaurant',\n",
       " 7255: 'west',\n",
       " 3893: 'los',\n",
       " 222: 'angeles',\n",
       " 945: 'called',\n",
       " 4861: 'pho',\n",
       " 409: 'authentic',\n",
       " 5314: 'read',\n",
       " 7429: 'yelp',\n",
       " 4506: 'number',\n",
       " 5778: 'second',\n",
       " 525: 'bathroom',\n",
       " 3768: 'legit',\n",
       " 1983: 'double',\n",
       " 6490: 'supply',\n",
       " 1276: 'closet',\n",
       " 2698: 'gallons',\n",
       " 676: 'bleach',\n",
       " 374: 'atm',\n",
       " 3946: 'machine',\n",
       " 2860: 'grandma',\n",
       " 2796: 'glaucoma',\n",
       " 4377: 'napping',\n",
       " 1495: 'corner',\n",
       " 7163: 'wait',\n",
       " 6271: 'staff',\n",
       " 3752: 'leave',\n",
       " 1717: 'deaf',\n",
       " 2184: 'emotionally',\n",
       " 22: 'abused',\n",
       " 6793: 'total',\n",
       " 617: 'big',\n",
       " 3120: 'hippies',\n",
       " 454: 'backpack',\n",
       " 6182: 'southeast',\n",
       " 335: 'asia',\n",
       " 7435: 'yoga',\n",
       " 437: 'ayahuasca',\n",
       " 1067: 'ceremonies',\n",
       " 5986: 'silent',\n",
       " 5489: 'retreats',\n",
       " 4780: 'pay',\n",
       " 5960: 'shut',\n",
       " 7238: 'weekend',\n",
       " 2805: 'glutenfree',\n",
       " 4091: 'means',\n",
       " 800: 'bread',\n",
       " 6585: 'tastes',\n",
       " 2639: 'freerange',\n",
       " 1153: 'chewbacca',\n",
       " 3776: 'lesbian',\n",
       " 6697: 'thousand',\n",
       " 1663: 'daily',\n",
       " 2452: 'fiber',\n",
       " 6235: 'spoken',\n",
       " 7362: 'word',\n",
       " 4963: 'poetry',\n",
       " 5226: 'queef',\n",
       " 5916: 'shitty',\n",
       " 4961: 'poem',\n",
       " 6492: 'supporting',\n",
       " 933: 'caitlyn',\n",
       " 3501: 'jenner',\n",
       " 2691: 'funny',\n",
       " 3121: 'hippydippy',\n",
       " 1957: 'doing',\n",
       " 3311: 'impression',\n",
       " 5757: 'scrolls',\n",
       " 7174: 'wall',\n",
       " 860: 'buddha',\n",
       " 4895: 'piggy',\n",
       " 495: 'bank',\n",
       " 4891: 'pier',\n",
       " 3307: 'imports',\n",
       " 5158: 'providing',\n",
       " 2821: 'good',\n",
       " 2441: 'feng',\n",
       " 5958: 'shui',\n",
       " 3210: 'house',\n",
       " 7423: 'years',\n",
       " 6119: 'sneaking',\n",
       " 6513: 'suspicion',\n",
       " 5144: 'propose',\n",
       " 5073: 'pressuring',\n",
       " 7156: 'wacky',\n",
       " 3421: 'intuition',\n",
       " 5143: 'proposals',\n",
       " 7366: 'work',\n",
       " 3320: 'incept',\n",
       " 3267: 'idea',\n",
       " 4004: 'mans',\n",
       " 4756: 'passively',\n",
       " 1952: 'doesnt',\n",
       " 4141: 'message',\n",
       " 2344: 'extremely',\n",
       " 114: 'aggressively',\n",
       " 6700: 'threaten',\n",
       " 60: 'actually',\n",
       " 3754: 'leaving',\n",
       " 4564: 'old',\n",
       " 3710: 'late',\n",
       " 4437: 'new',\n",
       " 6295: 'start',\n",
       " 4000: 'manipulation',\n",
       " 1654: 'cycle',\n",
       " 6344: 'stick',\n",
       " 2571: 'focus',\n",
       " 6848: 'trapping',\n",
       " 4362: 'nag',\n",
       " 4652: 'outta',\n",
       " 7218: 'weak',\n",
       " 1048: 'caves',\n",
       " 2763: 'gets',\n",
       " 2421: 'fed',\n",
       " 2484: 'fine',\n",
       " 4030: 'marry',\n",
       " 5145: 'proposed',\n",
       " 3882: 'look',\n",
       " 2285: 'exact',\n",
       " 5537: 'ring',\n",
       " 7185: 'wanted',\n",
       " 4072: 'maybe',\n",
       " 4907: 'pinterest',\n",
       " 4686: 'page',\n",
       " 5830: 'sent',\n",
       " 595: 'best',\n",
       " 2649: 'friend',\n",
       " 5820: 'send',\n",
       " 4908: 'pinterested',\n",
       " 2207: 'engaged',\n",
       " 5686: 'saturday',\n",
       " 762: 'bought',\n",
       " 2015: 'dress',\n",
       " 2580: 'following',\n",
       " 6916: 'tuesday',\n",
       " 6877: 'tried',\n",
       " 5318: 'ready',\n",
       " 5542: 'ripe',\n",
       " 5594: 'rotten',\n",
       " 489: 'banana',\n",
       " 6505: 'surprised',\n",
       " 4557: 'offstage',\n",
       " 1387: 'completely',\n",
       " 5352: 'recognize',\n",
       " 4837: 'personality',\n",
       " 6140: 'soft',\n",
       " 4513: 'nurturing',\n",
       " 1964: 'domestic',\n",
       " 7257: 'weve',\n",
       " 3465: 'ive',\n",
       " 4679: 'packed',\n",
       " 3929: 'lunch',\n",
       " 6006: 'single',\n",
       " 3064: 'hed',\n",
       " 1791: 'dependent',\n",
       " 2848: 'graduated',\n",
       " 2423: 'feed',\n",
       " 2824: 'goodness',\n",
       " 3053: 'heart',\n",
       " 3427: 'investment',\n",
       " 2480: 'financial',\n",
       " 5316: 'reading',\n",
       " 738: 'book',\n",
       " 5902: 'sheryl',\n",
       " 5666: 'sandberg',\n",
       " 5903: 'shes',\n",
       " 1479: 'coo',\n",
       " 2353: 'facebook',\n",
       " 7410: 'wrote',\n",
       " 5535: 'riled',\n",
       " 994: 'careers',\n",
       " 6569: 'talking',\n",
       " 1079: 'challenge',\n",
       " 6014: 'sit',\n",
       " 6554: 'table',\n",
       " 5545: 'rise',\n",
       " 3745: 'lean',\n",
       " 3806: 'lie',\n",
       " 7184: 'want',\n",
       " 2437: 'feminism',\n",
       " 3526: 'job',\n",
       " 7050: 'used',\n",
       " 6088: 'smart',\n",
       " 1460: 'continue',\n",
       " 2064: 'dumb',\n",
       " 1061: 'century',\n",
       " 2913: 'guess',\n",
       " 6316: 'stay',\n",
       " 6109: 'snacks',\n",
       " 7207: 'watch',\n",
       " 2161: 'ellen',\n",
       " 6435: 'stupid',\n",
       " 5319: 'real',\n",
       " 653: 'bitch',\n",
       " 5616: 'ruined',\n",
       " 2317: 'expected',\n",
       " 3049: 'hear',\n",
       " 4870: 'phrase',\n",
       " 1984: 'doubleincome',\n",
       " 3212: 'household',\n",
       " 7038: 'upset',\n",
       " 1363: 'comments',\n",
       " 4616: 'options',\n",
       " 2636: 'free',\n",
       " 7025: 'unscheduled',\n",
       " 7027: 'unsupervised',\n",
       " 3306: 'importantly',\n",
       " 6236: 'sponsored',\n",
       " 5914: 'shittier',\n",
       " 2583: 'food',\n",
       " 2087: 'earn',\n",
       " 3454: 'ita',\n",
       " 7170: 'walk',\n",
       " 6680: 'theyll',\n",
       " 3559: 'judgmental',\n",
       " 3215: 'housewives',\n",
       " 6386: 'street',\n",
       " 3214: 'housewife',\n",
       " 7172: 'walking',\n",
       " 4044: 'massages',\n",
       " 3928: 'lululemon',\n",
       " 4711: 'pants',\n",
       " 2746: 'genius',\n",
       " 5487: 'retiredi',\n",
       " 7403: 'write',\n",
       " 2644: 'fresh',\n",
       " 713: 'boat',\n",
       " 6: 'abc',\n",
       " 2877: 'great',\n",
       " 1546: 'coworkers',\n",
       " 7405: 'writing',\n",
       " 6635: 'terms',\n",
       " 3527: 'jobs',\n",
       " 4553: 'office',\n",
       " 6034: 'skin',\n",
       " 5773: 'seat',\n",
       " 7049: 'use',\n",
       " 6766: 'toilet',\n",
       " 4713: 'paper',\n",
       " 1539: 'cover',\n",
       " 6739: 'times',\n",
       " 5643: 'sadass',\n",
       " 4085: 'meal',\n",
       " 4584: 'oneply',\n",
       " 5197: 'purposely',\n",
       " 1861: 'difficult',\n",
       " 5179: 'pull',\n",
       " 6905: 'try',\n",
       " 5303: 'ration',\n",
       " 1369: 'communist',\n",
       " 2128: 'effective',\n",
       " 1763: 'dehydrates',\n",
       " 7328: 'wiping',\n",
       " 1803: 'desert',\n",
       " 3842: 'literally',\n",
       " 6193: 'spat',\n",
       " 1710: 'days',\n",
       " 115: 'ago',\n",
       " 3943: 'macgyver',\n",
       " 443: 'baby',\n",
       " 7326: 'wipe',\n",
       " 4245: 'moisten',\n",
       " 449: 'backfired',\n",
       " 2487: 'fingers',\n",
       " 834: 'broke',\n",
       " 1866: 'digitally',\n",
       " 6347: 'stimulated',\n",
       " 1970: 'doo',\n",
       " 2488: 'finish',\n",
       " 5631: 'rushed',\n",
       " 4719: 'paranoid',\n",
       " 5923: 'shoes',\n",
       " 6991: 'underneath',\n",
       " 6277: 'stall',\n",
       " 1535: 'courtneys',\n",
       " 3839: 'listening',\n",
       " 7165: 'waiting',\n",
       " 6740: 'timing',\n",
       " 3247: 'hurry',\n",
       " 2428: 'feels',\n",
       " 935: 'caked',\n",
       " 3877: 'long',\n",
       " 1684: 'dare',\n",
       " 5744: 'scratch',\n",
       " 6997: 'underwear',\n",
       " 2196: 'end',\n",
       " 3885: 'looks',\n",
       " 2827: 'goonies',\n",
       " 4322: 'muffle',\n",
       " 7384: 'worry',\n",
       " 7096: 'velocity',\n",
       " 6259: 'squeeze',\n",
       " 1132: 'cheeks',\n",
       " 6497: 'sure',\n",
       " 6074: 'slow',\n",
       " 6323: 'steady',\n",
       " 4676: 'pace',\n",
       " 7019: 'unpredictable',\n",
       " 4473: 'noise',\n",
       " 6456: 'suddenly',\n",
       " 2246: 'escapes',\n",
       " 828: 'brings',\n",
       " 1746: 'deep',\n",
       " 5876: 'shame',\n",
       " 695: 'blow',\n",
       " 2107: 'echo',\n",
       " 5500: 'reverberate',\n",
       " 2201: 'ends',\n",
       " 2959: 'hallways',\n",
       " 7210: 'watching',\n",
       " 4432: 'netflix',\n",
       " 3437: 'ipad',\n",
       " 747: 'boring',\n",
       " 5450: 'repressed',\n",
       " 5913: 'shits',\n",
       " 3838: 'listen',\n",
       " 4960: 'podcasts',\n",
       " 4925: 'planet',\n",
       " 7187: 'wantyou',\n",
       " 1930: 'distracting',\n",
       " 3894: 'lose',\n",
       " 5471: 'respect',\n",
       " 3142: 'holds',\n",
       " 6171: 'sort',\n",
       " 1577: 'credence',\n",
       " 3050: 'heard',\n",
       " 4426: 'nerve',\n",
       " 490: 'bananas',\n",
       " 2883: 'green',\n",
       " 479: 'ballet',\n",
       " 2532: 'flats',\n",
       " 2407: 'fatherinlaw',\n",
       " 6016: 'sitdown',\n",
       " 5346: 'recently',\n",
       " 6567: 'talk',\n",
       " 5157: 'provide',\n",
       " 1163: 'children',\n",
       " 5102: 'privileged',\n",
       " 1161: 'childhood',\n",
       " 6618: 'telling',\n",
       " 1471: 'conversation',\n",
       " 6158: 'son',\n",
       " 6993: 'understand',\n",
       " 2088: 'earning',\n",
       " 1186: 'choose',\n",
       " 5479: 'rest',\n",
       " 1194: 'chose',\n",
       " 5130: 'promise',\n",
       " 2086: 'early',\n",
       " 5488: 'retirement',\n",
       " 4092: 'meant',\n",
       " 1856: 'dieting',\n",
       " 2104: 'eating',\n",
       " 2648: 'fried',\n",
       " 1156: 'chicken',\n",
       " 2677: 'fulfilling',\n",
       " 1814: 'destiny',\n",
       " 1211: 'circle',\n",
       " 2349: 'eyelashes',\n",
       " 4317: 'mrs',\n",
       " 4682: 'pacman',\n",
       " 3781: 'lets',\n",
       " 5364: 'redecoratei',\n",
       " 1913: 'disgusting',\n",
       " 4842: 'pervert',\n",
       " 2894: 'gross',\n",
       " 2477: 'filthy',\n",
       " 228: 'animal',\n",
       " 6296: 'started',\n",
       " 5008: 'porn',\n",
       " 7446: 'young',\n",
       " 106: 'age',\n",
       " 2994: 'happens',\n",
       " 7455: 'yyou',\n",
       " 5970: 'sicker',\n",
       " 3288: 'images',\n",
       " 1565: 'crave',\n",
       " 3407: 'internet',\n",
       " 7444: 'youi',\n",
       " 3275: 'idiot',\n",
       " 5324: 'realize',\n",
       " 7251: 'went',\n",
       " 1558: 'craigslist',\n",
       " 5024: 'posted',\n",
       " 6745: 'tiny',\n",
       " 2435: 'female',\n",
       " 5791: 'seeking',\n",
       " 206: 'anal',\n",
       " 1560: 'crash',\n",
       " 3981: 'male',\n",
       " 3040: 'heads',\n",
       " 7011: 'universe',\n",
       " 6001: 'simultaneously',\n",
       " 2336: 'explode',\n",
       " 2630: 'freaked',\n",
       " 5713: 'scared',\n",
       " 4690: 'pain',\n",
       " 131: 'aint',\n",
       " 7215: 'wax',\n",
       " 2348: 'eyebrows',\n",
       " 6173: 'sorts',\n",
       " 1570: 'crazy',\n",
       " 1842: 'dick',\n",
       " 2277: 'evil',\n",
       " 5783: 'secrets',\n",
       " 3808: 'lies',\n",
       " 5835: 'sephora',\n",
       " 5210: 'puts',\n",
       " 6688: 'thinking',\n",
       " 1658: 'dad',\n",
       " 1700: 'dave',\n",
       " 2269: 'eventually',\n",
       " 1091: 'change',\n",
       " 1123: 'cheat',\n",
       " 3404: 'interesting',\n",
       " 3144: 'holes',\n",
       " 7445: 'youll',\n",
       " 5862: 'sexually',\n",
       " 54: 'active',\n",
       " 5481: 'result',\n",
       " 3843: 'little',\n",
       " 652: 'bit',\n",
       " 6393: 'stretched',\n",
       " 2479: 'finally',\n",
       " 2434: 'felt',\n",
       " 1088: 'chance',\n",
       " 3961: 'magical',\n",
       " 2392: 'fantasy',\n",
       " 5021: 'possible',\n",
       " 1897: 'discover',\n",
       " 5147: 'prostate',\n",
       " 1434: 'conqueror',\n",
       " 3027: 'havent',\n",
       " 6859: 'treat',\n",
       " 6781: 'tonight',\n",
       " 3845: 'live',\n",
       " 7440: 'yolo',\n",
       " 6116: 'sneak',\n",
       " 5206: 'pushpush',\n",
       " 6929: 'tushtush',\n",
       " 364: 'atari',\n",
       " 5466: 'resistance',\n",
       " 6262: 'squirmy',\n",
       " 7381: 'wormy',\n",
       " 6720: 'thumb',\n",
       " 6448: 'succeed',\n",
       " 2727: 'gay',\n",
       " 2414: 'fear',\n",
       " 6927: 'turns',\n",
       " 2250: 'especially',\n",
       " 4150: 'metamorphosizes',\n",
       " 4945: 'pleasure',\n",
       " 2346: 'eye',\n",
       " 1898: 'discovered',\n",
       " 4466: 'nirvana',\n",
       " 3682: 'lady',\n",
       " 1267: 'clit',\n",
       " 2351: 'eyes',\n",
       " 3891: 'lord',\n",
       " 5536: 'rim',\n",
       " 7001: 'unfortunately',\n",
       " 2634: 'freaky',\n",
       " 341: 'asked',\n",
       " 6191: 'spank',\n",
       " 1951: 'does',\n",
       " 21: 'abuse',\n",
       " 6414: 'strongheaded',\n",
       " 3906: 'loudmouthed',\n",
       " 1063: 'ceos',\n",
       " 5597: 'roughed',\n",
       " 7186: 'wants',\n",
       " 1466: 'control',\n",
       " 5547: 'risk',\n",
       " 1181: 'choke',\n",
       " 6748: 'tired',\n",
       " 750: 'boss',\n",
       " 556: 'bedroom',\n",
       " 4290: 'motherfucker',\n",
       " 3298: 'impact',\n",
       " 487: 'ban',\n",
       " 752: 'bossy',\n",
       " 2154: 'elementary',\n",
       " 5726: 'schools',\n",
       " 5859: 'sexist',\n",
       " 780: 'boys',\n",
       " 3386: 'instead',\n",
       " 5705: 'saying',\n",
       " 6494: 'supposed',\n",
       " 2301: 'executive',\n",
       " 3741: 'leadership',\n",
       " 6033: 'skills',\n",
       " 5599: 'roundabout',\n",
       " 1627: 'cunt',\n",
       " 2481: 'financially',\n",
       " 1069: 'certain',\n",
       " 4965: 'point',\n",
       " 1527: 'couple',\n",
       " 1350: 'comfortably',\n",
       " 94: 'afford',\n",
       " 6059: 'sliced',\n",
       " 3993: 'mango',\n",
       " 2584: 'foods',\n",
       " 3327: 'income',\n",
       " 785: 'bracket',\n",
       " 6406: 'striving',\n",
       " 7452: 'youve',\n",
       " 4370: 'named',\n",
       " 4468: 'noah',\n",
       " 5339: 'rebecca',\n",
       " 3648: 'kiwi',\n",
       " 1681: 'danielle',\n",
       " 4904: 'pineapple',\n",
       " 10: 'able',\n",
       " 6410: 'stroll',\n",
       " 5974: 'sidewalk',\n",
       " 5224: 'quarter',\n",
       " 5089: 'princess',\n",
       " 7051: 'useful',\n",
       " 88: 'advice',\n",
       " 841: 'brothers',\n",
       " 6013: 'sisters',\n",
       " 4693: 'paintballing',\n",
       " 7119: 'vietnam',\n",
       " 7104: 'veteran',\n",
       " 5848: 'seven',\n",
       " 2945: 'half',\n",
       " 4271: 'months',\n",
       " 5057: 'pregnant',\n",
       " 5299: 'rare',\n",
       " 1352: 'comic',\n",
       " 4826: 'perform',\n",
       " 1353: 'comics',\n",
       " 2737: 'generally',\n",
       " 1017: 'case',\n",
       " 7237: 'week',\n",
       " 444: 'babys',\n",
       " 4888: 'piece',\n",
       " 240: 'annoying',\n",
       " 1662: 'dads',\n",
       " 398: 'audience',\n",
       " 3107: 'hilarious',\n",
       " 3273: 'identify',\n",
       " 2377: 'fame',\n",
       " 6532: 'swells',\n",
       " 5403: 'relatable',\n",
       " 6455: 'sudden',\n",
       " 1103: 'chapping',\n",
       " 4464: 'nipples',\n",
       " 2424: 'feeding',\n",
       " 7224: 'wearing',\n",
       " 2662: 'frozen',\n",
       " 1838: 'diaper',\n",
       " 4409: 'needs',\n",
       " 5953: 'shredding',\n",
       " 2990: 'happen',\n",
       " 6283: 'standup',\n",
       " 6450: 'successful',\n",
       " 2384: 'famous',\n",
       " 1895: 'discouraged',\n",
       " 3028: 'having',\n",
       " 3620: 'kid',\n",
       " 3687: 'lame',\n",
       " 6317: 'stayathome',\n",
       " 2011: 'dream',\n",
       " 6973: 'ultimate',\n",
       " 7351: 'won',\n",
       " 3660: 'knowanother',\n",
       " 1896: 'discouraging',\n",
       " 6852: 'travel',\n",
       " 380: 'attached',\n",
       " 1855: 'dies',\n",
       " 1757: 'definitely',\n",
       " 2909: 'guaranteed',\n",
       " 634: 'billion',\n",
       " 6928: 'turtle',\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"shit\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"hes\" + 0.005*\"fuck\" + 0.005*\"good\" + 0.005*\"life\" + 0.004*\"man\" + 0.004*\"going\" + 0.004*\"didnt\"'),\n",
       " (1,\n",
       "  '0.009*\"fucking\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.005*\"went\" + 0.005*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"shit\" + 0.005*\"thing\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"fucking\" + 0.006*\"fuck\" + 0.006*\"going\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.005*\"cause\" + 0.005*\"good\" + 0.005*\"thing\" + 0.005*\"really\" + 0.005*\"life\"'),\n",
       " (1,\n",
       "  '0.007*\"love\" + 0.007*\"bo\" + 0.006*\"stuff\" + 0.006*\"repeat\" + 0.005*\"want\" + 0.004*\"fucking\" + 0.004*\"cos\" + 0.004*\"eye\" + 0.004*\"um\" + 0.004*\"contact\"'),\n",
       " (2,\n",
       "  '0.008*\"fucking\" + 0.008*\"shit\" + 0.006*\"didnt\" + 0.005*\"say\" + 0.005*\"hes\" + 0.005*\"little\" + 0.005*\"fuck\" + 0.005*\"did\" + 0.004*\"theyre\" + 0.004*\"day\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"fucking\" + 0.008*\"fuck\" + 0.007*\"shit\" + 0.006*\"good\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"man\" + 0.005*\"say\" + 0.005*\"theres\" + 0.005*\"hes\"'),\n",
       " (1,\n",
       "  '0.001*\"fucking\" + 0.000*\"fuck\" + 0.000*\"want\" + 0.000*\"hes\" + 0.000*\"going\" + 0.000*\"man\" + 0.000*\"shit\" + 0.000*\"cause\" + 0.000*\"goes\" + 0.000*\"say\"'),\n",
       " (2,\n",
       "  '0.006*\"say\" + 0.006*\"fucking\" + 0.006*\"going\" + 0.005*\"want\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"did\" + 0.004*\"little\" + 0.004*\"theyre\" + 0.004*\"shit\"'),\n",
       " (3,\n",
       "  '0.008*\"shit\" + 0.006*\"ok\" + 0.005*\"lot\" + 0.005*\"gotta\" + 0.005*\"wanna\" + 0.004*\"husband\" + 0.004*\"cause\" + 0.004*\"day\" + 0.004*\"women\" + 0.003*\"pregnant\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #2 (Nouns Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number\n",
    "Tag\n",
    "Description\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>macdonald farm e i o farm pig e i i snort macd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats davis whats home i netflix la york i son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies gentlemen joe fuck thanks phone fuckfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "bo       macdonald farm e i o farm pig e i i snort macd...\n",
       "dave     jokes living stare work profound train thought...\n",
       "hasan    whats davis whats home i netflix la york i son...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fuck thanks phone fuckfac...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                input=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                 'afterwards', 'again', 'against', 'all',\n",
       "                                 'almost', 'alone', 'along', 'already', 'also',\n",
       "                                 'although', 'always', 'am', 'among', 'amongst',\n",
       "                                 'amoungst', 'amount', 'an', 'and', 'another',\n",
       "                                 'any', 'anyhow', 'anyone', 'anything',\n",
       "                                 'anyway', 'anywhere', ...}),\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  ability  \\\n",
       "ali                 0                 0          0    0    1     0        0   \n",
       "anthony             0                 0          0    0    0     0        0   \n",
       "bill                0                 0          0    0    0     1        0   \n",
       "bo                  1                 1          1    0    0     0        1   \n",
       "dave                0                 0          0    0    0     0        0   \n",
       "hasan               0                 0          0    0    0     0        0   \n",
       "jim                 0                 0          0    0    0     0        0   \n",
       "joe                 0                 0          0    0    0     0        0   \n",
       "john                0                 0          0    0    0     0        0   \n",
       "louis               0                 0          0    3    0     0        0   \n",
       "mike                0                 0          0    0    0     0        0   \n",
       "ricky               0                 0          0    0    0     0        1   \n",
       "\n",
       "         abortion  abortions  abuse  ...  yummy  ze  zealand  zee  zeppelin  \\\n",
       "ali             0          0      0  ...      0   0        0    0         0   \n",
       "anthony         2          0      0  ...      0   0       10    0         0   \n",
       "bill            0          0      0  ...      0   1        0    0         0   \n",
       "bo              0          0      0  ...      0   0        0    0         0   \n",
       "dave            0          1      0  ...      0   0        0    0         0   \n",
       "hasan           0          0      0  ...      0   0        0    1         0   \n",
       "jim             0          0      0  ...      0   0        0    0         0   \n",
       "joe             0          0      1  ...      0   0        0    0         0   \n",
       "john            0          0      0  ...      0   0        0    0         0   \n",
       "louis           0          0      0  ...      0   0        0    0         0   \n",
       "mike            0          0      0  ...      0   0        0    0         2   \n",
       "ricky           0          0      0  ...      1   0        0    0         0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 4635 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "# TDM -> SPARSE -> CORPUS\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"day\" + 0.010*\"thing\" + 0.007*\"man\" + 0.007*\"fuck\" + 0.007*\"shit\" + 0.006*\"hes\" + 0.006*\"cause\" + 0.006*\"way\" + 0.006*\"guy\" + 0.006*\"life\"'),\n",
       " (1,\n",
       "  '0.008*\"shit\" + 0.008*\"life\" + 0.007*\"hes\" + 0.007*\"man\" + 0.006*\"lot\" + 0.006*\"way\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.005*\"day\" + 0.005*\"guy\"')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"way\" + 0.007*\"stuff\" + 0.006*\"clinton\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"day\" + 0.006*\"repeat\" + 0.005*\"mom\" + 0.005*\"hes\" + 0.005*\"eye\"'),\n",
       " (1,\n",
       "  '0.009*\"dad\" + 0.006*\"life\" + 0.006*\"shes\" + 0.006*\"night\" + 0.006*\"point\" + 0.006*\"school\" + 0.006*\"way\" + 0.006*\"parents\" + 0.005*\"cause\" + 0.005*\"girl\"'),\n",
       " (2,\n",
       "  '0.011*\"shit\" + 0.011*\"thing\" + 0.010*\"day\" + 0.009*\"man\" + 0.009*\"fuck\" + 0.009*\"life\" + 0.008*\"hes\" + 0.007*\"cause\" + 0.007*\"lot\" + 0.007*\"guy\"')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"joke\" + 0.008*\"thing\" + 0.008*\"years\" + 0.008*\"day\" + 0.007*\"hes\" + 0.007*\"id\" + 0.006*\"things\" + 0.005*\"jenner\" + 0.005*\"nuts\" + 0.005*\"woman\"'),\n",
       " (1,\n",
       "  '0.011*\"dad\" + 0.008*\"day\" + 0.007*\"life\" + 0.007*\"joke\" + 0.006*\"school\" + 0.006*\"mom\" + 0.006*\"stuff\" + 0.006*\"man\" + 0.006*\"way\" + 0.005*\"shes\"'),\n",
       " (2,\n",
       "  '0.008*\"cause\" + 0.008*\"way\" + 0.008*\"day\" + 0.008*\"thing\" + 0.008*\"guy\" + 0.007*\"gon\" + 0.007*\"shit\" + 0.007*\"hes\" + 0.007*\"kind\" + 0.006*\"hey\"'),\n",
       " (3,\n",
       "  '0.012*\"shit\" + 0.011*\"man\" + 0.010*\"fuck\" + 0.010*\"thing\" + 0.009*\"life\" + 0.009*\"cause\" + 0.009*\"lot\" + 0.009*\"day\" + 0.009*\"women\" + 0.008*\"hes\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>old macdonald farm e i i o farm pig e i i snor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats davis whats im home i netflix special la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies gentlemen joe fuck san francisco thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "bo       old macdonald farm e i i o farm pig e i i snor...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "hasan    whats davis whats im home i netflix special la...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fuck san francisco thanks...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>éclair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 5587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  \\\n",
       "ali           0             0                 0          0    0    1     0   \n",
       "anthony       0             0                 0          0    0    0     0   \n",
       "bill          1             0                 0          0    0    0     1   \n",
       "bo            0             1                 1          1    0    0     0   \n",
       "dave          0             0                 0          0    0    0     0   \n",
       "hasan         0             0                 0          0    0    0     0   \n",
       "jim           0             0                 0          0    0    0     0   \n",
       "joe           0             0                 0          0    0    0     0   \n",
       "john          0             0                 0          0    0    0     0   \n",
       "louis         0             0                 0          0    3    0     0   \n",
       "mike          0             0                 0          0    0    0     0   \n",
       "ricky         0             0                 0          0    0    0     0   \n",
       "\n",
       "         ability  abject  able  ...  ze  zealand  zee  zeppelin  zero  \\\n",
       "ali            0       0     2  ...   0        0    0         0     0   \n",
       "anthony        0       0     0  ...   0       10    0         0     0   \n",
       "bill           0       0     1  ...   1        0    0         0     0   \n",
       "bo             1       0     0  ...   0        0    0         0     1   \n",
       "dave           0       0     0  ...   0        0    0         0     0   \n",
       "hasan          0       0     1  ...   0        0    2         0     0   \n",
       "jim            0       0     1  ...   0        0    0         0     0   \n",
       "joe            0       0     2  ...   0        0    0         0     0   \n",
       "john           0       0     3  ...   0        0    0         0     0   \n",
       "louis          0       0     1  ...   0        0    0         0     0   \n",
       "mike           0       0     0  ...   0        0    0         2     0   \n",
       "ricky          1       1     2  ...   0        0    0         0     0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  éclair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 5587 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"mom\" + 0.005*\"parents\" + 0.003*\"hasan\" + 0.003*\"clinton\" + 0.003*\"jenny\" + 0.003*\"friend\" + 0.003*\"york\" + 0.003*\"door\" + 0.003*\"president\" + 0.002*\"dick\"'),\n",
       " (1,\n",
       "  '0.006*\"joke\" + 0.003*\"gun\" + 0.003*\"son\" + 0.003*\"jokes\" + 0.003*\"ass\" + 0.003*\"bo\" + 0.002*\"hell\" + 0.002*\"um\" + 0.002*\"gay\" + 0.002*\"guns\"')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"comedy\" + 0.003*\"jokes\" + 0.003*\"stupid\" + 0.003*\"eye\" + 0.003*\"sad\" + 0.003*\"anthony\"'),\n",
       " (1,\n",
       "  '0.005*\"mom\" + 0.004*\"hasan\" + 0.004*\"ahah\" + 0.003*\"son\" + 0.003*\"husband\" + 0.003*\"gay\" + 0.003*\"parents\" + 0.003*\"door\" + 0.003*\"nigga\" + 0.003*\"brown\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.004*\"ass\" + 0.004*\"dog\" + 0.004*\"clinton\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"parents\" + 0.003*\"wife\" + 0.003*\"tit\" + 0.003*\"food\"')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"joke\" + 0.004*\"parents\" + 0.004*\"mom\" + 0.004*\"comedy\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"jenny\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"class\"'),\n",
       " (1,\n",
       "  '0.007*\"ahah\" + 0.005*\"husband\" + 0.005*\"gay\" + 0.005*\"nigga\" + 0.004*\"ok\" + 0.004*\"asian\" + 0.003*\"pregnant\" + 0.003*\"son\" + 0.003*\"young\" + 0.003*\"oj\"'),\n",
       " (2,\n",
       "  '0.005*\"joke\" + 0.003*\"stupid\" + 0.003*\"nuts\" + 0.003*\"hell\" + 0.003*\"religion\" + 0.003*\"jenner\" + 0.003*\"dick\" + 0.003*\"jesus\" + 0.003*\"dead\" + 0.003*\"gun\"'),\n",
       " (3,\n",
       "  '0.007*\"clinton\" + 0.006*\"mom\" + 0.005*\"tit\" + 0.005*\"cow\" + 0.005*\"parents\" + 0.005*\"wife\" + 0.004*\"ha\" + 0.004*\"president\" + 0.004*\"dog\" + 0.004*\"food\"')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"joke\" + 0.005*\"nuts\" + 0.005*\"jenner\" + 0.005*\"chimp\" + 0.005*\"hampstead\" + 0.004*\"rape\" + 0.004*\"twitter\" + 0.004*\"tweet\" + 0.004*\"jane\" + 0.004*\"doctor\"'),\n",
       " (1,\n",
       "  '0.008*\"mom\" + 0.006*\"parents\" + 0.005*\"joke\" + 0.004*\"hasan\" + 0.004*\"clinton\" + 0.004*\"anthony\" + 0.003*\"york\" + 0.003*\"tit\" + 0.003*\"cow\" + 0.003*\"entire\"'),\n",
       " (2,\n",
       "  '0.004*\"jenny\" + 0.003*\"religion\" + 0.003*\"husband\" + 0.003*\"stupid\" + 0.003*\"sense\" + 0.003*\"dick\" + 0.003*\"ass\" + 0.003*\"texas\" + 0.002*\"morning\" + 0.002*\"ok\"'),\n",
       " (3,\n",
       "  '0.006*\"bo\" + 0.005*\"repeat\" + 0.005*\"ahah\" + 0.005*\"guns\" + 0.005*\"um\" + 0.004*\"son\" + 0.004*\"gay\" + 0.004*\"eye\" + 0.004*\"contact\" + 0.003*\"ass\"')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: mom, parents\n",
    "* Topic 1: husband, wife\n",
    "* Topic 2: guns\n",
    "* Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'ali'),\n",
       " (1, 'anthony'),\n",
       " (2, 'bill'),\n",
       " (3, 'bo'),\n",
       " (3, 'dave'),\n",
       " (1, 'hasan'),\n",
       " (3, 'jim'),\n",
       " (2, 'joe'),\n",
       " (1, 'john'),\n",
       " (1, 'louis'),\n",
       " (2, 'mike'),\n",
       " (0, 'ricky')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
